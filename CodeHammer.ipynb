{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed3cc11-5f24-43af-9282-16b6d183d02c",
   "metadata": {},
   "source": [
    "October 31, 2023\n",
    "\n",
    "Running this example on my brand spanking new 4090!\n",
    "\n",
    "[How to Load LLama 13b for Inference on a single RTX 4090](https://codehammer.io/how-to-load-llama-13b-for-inference-on-a-single-rtx-4090/)\n",
    "\n",
    "And no, this notebook is NOT part of the repo ...\n",
    "\n",
    "[Natural Language Processing with Transformers by Lewis Tunstall, Leandro von Werra, Thomas Wolf.](https://github.com/nlp-with-transformers/notebooks)\n",
    "\n",
    "docker container start hfpt_Oct28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d5330-298e-4102-b48d-c9bd3e8b2860",
   "metadata": {},
   "source": [
    "To copy downloaded models from this docker container to some external source, you need to know the id of this container plus the location of the files. For these Meta LLMs, the source folder is '/home/rob/Data2/huggingface/transformers' and the container id is 'c9b676310ea0'.\n",
    "\n",
    "So to copy a model from this folder, drop into a terminal window and type 'ls /home/rob/Data2/huggingface/transformers' to see what is available, then from an external terminal window and with the container running (actually it doesn't need to be running!), you can type 'docker cp c9b676310ea0:/home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-chat-hf /home/rob/Data3/huggingface/transformers'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e6b7267-41a8-4e19-8513-8327b292d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2903e9-cdb4-4638-be32-b5d274bc218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So turns out this no longer exists!\n",
    "checkpoint = \"decapoda-research/llama-13b-hf\"\n",
    "\n",
    "# And there is now LLama2 ...\n",
    "checkpoint = \"meta-llama/Llama-2-7b-hf\"\n",
    "checkpoint = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "checkpoint = \"meta-llama/Llama-2-7b-chat-hf\" # This has been downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86513c98-e222-4c78-964e-549653ab3f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f58f8da96794fd29de890c1bf4f14b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.65 s, sys: 2.1 s, total: 6.75 s\n",
      "Wall time: 5.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint) # takes up 29.4gb of ram\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True) # wow! this only takes up 3.4gb of ram and 7660Mib of VRAM!\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True, device_map='auto') # hmm this fails\n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e0de7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0731d895d54831a832be3185c06371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.39 s, sys: 8.7 s, total: 15.1 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# how about we try the 13b model?\n",
    "checkpoint = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True) # Holy crap, this works! 13,948MiB of VRAM!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc50a93-512f-4c30-9a4d-089c30f9f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c72d8d-5ef5-4fd9-be1f-645d1d63bf11",
   "metadata": {},
   "source": [
    "These next 3 cells you should keep them, but don't run them ... this dumps the model into these 3 locally container folders under /home/rob/Data3.\n",
    "\n",
    "NOTHING gets written into the external '/home/rob/Data3' folder ... !\n",
    "\n",
    "I think I need to gain a much better understanding about docker volumes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d798a8b-c702-4623-b8cc-2a66154d2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WOW ... this does NOT save to the external Data3 drive, but saves it again into this container, creating a folder with the same name!\n",
    "# model.save_pretrained('/home/rob/Data3/huggingface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6fe247-407f-44ce-aba2-209c012ccb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('/home/rob/Data3/fuckthisshit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7654d9e5-21db-4bc2-9df2-408f2319e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('/home/rob/Data3/blowme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3f0b2-641f-486f-94a7-1164e6958135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmmm this does not fit on the 4090! ... damn!\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4580d-11be-496f-b5c2-1d26435ae11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "checkpoint = \"meta-llama/Llama-2-7b\" \n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# OSError: meta-llama/Llama-2-7b does not appear to have a file named config.json. Checkout 'https://huggingface.co/meta-llama/Llama-2-7b/main' for available files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc60f9-073c-4528-b4e7-2eea3d52f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "checkpoint = \"meta-llama/Llama-2-13b\" # Nope!\n",
    "checkpoint = \"meta-llama/Llama-2-13b-chat-hf\" # Takes up all 32gb of ram plus 24gb of swap space!\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d8b0f-39ab-4a40-97b3-78f3d2c307c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9c97d-27c1-49ef-ad4a-c7c4925071ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
